def test_of_significance(Y,Y_val,X,X_val,value,value2,maxiter=0):
  
  c=0
  
  while c == 0:
    
    logit_model=sm.Logit(Y,sm.add_constant(X))
    result=logit_model.fit(method='bfgs')
    
    logit_model=sm.Logit(Y_val,sm.add_constant(X_val))
    result_val=logit_model.fit(method='bfgs')

    dev_sign = result.params
    val_sign = result_val.params
      
    dev_sign_df = pd.DataFrame(dev_sign,columns=['score'])
    val_sign_df = pd.DataFrame(val_sign,columns=['score'])
      
    sv=dev_sign_df[dev_sign_df['score']*val_sign_df['score']<0].index

    if len(sv) !=0:
      X=X.drop(axis=1,labels=sv)
      X_val = X_val.drop(axis=1,labels=sv)

    logit_model=sm.Logit(Y,sm.add_constant(X))
    result=logit_model.fit(method='bfgs')
    pvalue  = pd.DataFrame(result.pvalues,columns = ['Value'])

    dr=[]
    U = X.corr()
    for i in range(len(U.columns)-1):
      for j in range(i+1,len(U.columns)):
        if U.iloc[j,i] > value2:
          if pvalue['Value'][i]>pvalue['Value'][j]:
            dr.append(i)
          else:
            dr.append(j)
    dr=list(set(dr))

    X = X.drop(U.columns[dr],axis=1)
    X_val = X_val.drop(U.columns[dr],axis=1)
    
    logit_model=sm.Logit(Y,sm.add_constant(X))
    result=logit_model.fit(method='bfgs')
    pvalue  = pd.DataFrame(result.pvalues,columns = ['Value'])
    rv = pvalue[pvalue['Value']>value].index
    
    if len(rv) !=0:
      X=X.drop(axis=1,labels=rv)
      X_val =X_val.drop(axis=1,labels=rv)
      

    if len(list(rv)+list(sv)+list(dr))==0:
      c=1
  
  vec = TfidfVectorizer(ngram_range = (1,3), norm=None, use_idf = False, lowercase = True, decode_error='ignore',strip_accents='unicode', stop_words='english',binary=True,vocabulary=list(X.columns))
  
  return [X,X_val,vec]


# Feature Selection

def feature_selection(corpus,label,lexicon,n_k=200,rf_iter=10):
 
  vec = TfidfVectorizer(ngram_range = (1,3),norm=None,use_idf = False,lowercase = True,decode_error='ignore',strip_accents='unicode',stop_words='english',min_df=.01,max_df=.3,binary=True,max_features=1000)
  voc=[]
  for k in range(rf_iter):
  
    ind  = label[label==1].index
    ind1 = label[label==0].sample(3000).index
    
    X1 = vec.fit_transform(corpus.iloc[ind].append(corpus.iloc[ind1]))
    Y1 = pd.DataFrame(X1.todense()).fillna(0)
    label_rf = label.iloc[ind].append(label.iloc[ind1])
  
    mod = RandomForestClassifier(n_estimators=100,max_depth=10,max_features=25,bootstrap=True)
    mod = mod.fit(Y1,label_rf)
    SSS=list(vec.vocabulary_.keys())
    FI = pd.DataFrame([[SSS[s],mod.feature_importances_[s]] for s in range(len(mod.feature_importances_))])
    vocacb3 = FI.sort_values(by=1).iloc[-n_k:][0]
    n_vocab = [x for x in vocacb3 if x not in list(lexicon)]
    voc=list(set(voc+n_vocab))
  
  vocab1=list(lexicon)+voc
 
  vec = TfidfVectorizer(ngram_range = (1,3), norm=None, use_idf = False, lowercase = True, decode_error='ignore',strip_accents='unicode', stop_words='english',binary=True,vocabulary=vocab1)
  X1 = vec.fit_transform(corpus)
  Y1 = pd.DataFrame(X1.todense()).fillna(0)
  Y1.columns = list(vec.vocabulary_.keys())
  dc = [x for x in Y1.columns if Y1.sum()[x]==0]
  Y1=Y1.drop(dc,axis=1)
  Cor = [np.corrcoef(Y1[x],label)[0][1] for x in Y1.columns]
  cor = pd.DataFrame(Cor,index = Y1.columns)
  s = cor[cor[0]<0].index
  Y1 = Y1.drop(axis=1,labels=s)
 
  vec = TfidfVectorizer(ngram_range = (1,3), norm=None, use_idf = False, lowercase = True, decode_error='ignore',strip_accents='unicode', stop_words='english',binary=True,vocabulary=Y1.columns)
  vec.fit(corpus)
  return [Y1,vec]
  
  
  
  

# -*- coding: utf-8 -*-
"""
Created on Tue Nov 19 09:03:45 2019

@author: Dell
"""

import pandas as pd 
import re
import warnings
warnings.filterwarnings('ignore')
from nltk.corpus import wordnet
from  more_itertools import unique_everseen
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from nltk.stem.wordnet import WordNetLemmatizer
from nltk import pos_tag
import random
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle
from sklearn.metrics import confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC 
from sklearn.model_selection import GridSearchCV 
from sklearn.linear_model import LogisticRegression

news_old = pd.read_excel(r"CCIL_new_raw_data.xlsx")
news_old.columns = ["docid", "url", "source", "content", "title", "creation_date", "cp_1", "cp_2","cp_3","cp_4","cp_5","cp_6","cp_7","cp_8","cp_9","cp_10","cp_11","cp_12","cp_13","tag"] 

def NA_fill(df, start = 0):
    for i in range(start, len(df.columns)-1):
        col_name1 = df.columns[i]
        col_name2 = df.columns[i+1]
        df[col_name2] = df[col_name2].fillna(df[col_name1])
    return df

def rm_ascii(df):
    #looping over rows and columns
    for i in range(len(df)):
        for j in range(len(df.columns)):
            #run only for string columns
            if type(df.iloc[i,j]) == "unicode":
            #get apostrophe and remove rest of the characters
                df.iloc[i,j] = df.iloc[i,j].encode('ascii', "ignore")
    return df

def rm_whitespace(df, col_name):
# loop over to remove extra whitespaces, if any
    col_change = df[col_name]
    for i in range(len(df)):
        col_change[i] = " ".join(str(col_change[i]).split())
    df[col_name] = col_change
    return df

news = news_old.copy()
news1 = NA_fill(news,6) # starting from CP columns only
news1.head()

news2 = rm_ascii(news1)
news3 = news2.copy()

for i in ["content", "creation_date", "cp_1", "cp_2","cp_3","cp_4","cp_5","cp_6","cp_7","cp_8","cp_9","cp_10","cp_11","cp_12","cp_13"]:
    news3 = rm_whitespace(news3, i)

def rpl_cp(alias_df, alias_col, ref_col, col):
    for i in range(len(alias_df)):
        col = re.sub(r"\b"+str(alias_df[alias_col][i])+r"\b", alias_df[ref_col][i], str(col))
    return col  


def cleaning(col, alias_df, alias_col_lst, ref_col):
    #remove ascii
    alias_df[ref_col] = alias_df[ref_col].map(lambda x: x.lower())
    #col_1 = col.encode('utf-8').encode('ascii', 'ignore')
    col_2 = " ".join(str(col).split())
    col_3 = col_2.lower()
    col_4 = col_3
    for i in alias_col_lst:
        col_4 = rpl_cp(alias_df, i, ref_col, col_4)
    col_5 = col_4
    col_7 = rpl_cp(alias_df,"lower_cp","memberid", col_5)     
    return col_7

def rm_words(df, col_name, lst = ["ltd", "Ltd", "LTD", "Pvt", "PVT", "pvt", "Limited", "Private"]):
    #fill_na with blanks
    df[col_name] = df[col_name].fillna("blank")
    #looping over the list
    for i in lst:
        #remove dots, remove list word, remove space from right end, converting to lower
        df[col_name] = df[col_name].map(lambda x: x.replace(".", "").replace(i, "").lower())
    # loop over to remove extra whitespaces, if any
    for i in range(len(df)):
        for j in range(len(df.columns)):
            df.iloc[i,j] = " ".join(str(df.iloc[i,j]).split())
    return df

news1 = news.copy()
for i in ["title", "creation_date", "cp_1", "cp_2","cp_3","cp_4","cp_5","cp_6","cp_7","cp_8","cp_9","cp_10","cp_11","cp_12","cp_13"]:
    news1 = rm_words(news1, i)

lst_coop = ["coperative", "co-operative", "co operative", "cooperative", "co-op", "co op", "coop", "sahakari", "co.", "co"] #maintain this sequence
for i in ["title", "creation_date", "cp_1", "cp_2","cp_3","cp_4","cp_5","cp_6","cp_7","cp_8","cp_9","cp_10","cp_11","cp_12","cp_13"]:
    news1 = rm_words(news1, i, lst = lst_coop)

alias_df = pd.read_excel(r"aliases_preprocessed.xlsx", index_col=0)
alias_df['cp_length']=alias_df.cp_name.map(lambda x:len(x))

alias_df = alias_df.sort_values(by=['cp_length'], ascending=False)


writer = pd.ExcelWriter(r'aliases_preprocessed_new.xlsx')
alias_df.to_excel(writer,'Sheet1')
writer.save()

news1 = pd.read_excel(r"news_cleaned_new.xlsx")
alias_df = pd.read_excel(r"aliases_preprocessed_new.xlsx")


alias_df = alias_df.reset_index(drop=True)

alias_col_lst = ["cp_name", "alias_1", "alias_2", "alias_3", "alias_4"]
ref_col= 'memberid'

news1["final_cleaned_title"] = news1.title.map(lambda x:cleaning(x, alias_df, alias_col_lst, ref_col))
news1["final_cleaned_content"] = news1.content.map(lambda x:cleaning(x, alias_df, alias_col_lst, ref_col))



news1 = pd.read_excel(r"n_c_new.xlsx")

news1["merged_col"] = news1["final_cleaned_title"] + " " + news1["final_cleaned_title"] + " " + news1["final_cleaned_title"] + " " + news1["final_cleaned_title"] + " " + news1["final_cleaned_title"] + " " + news1["final_cleaned_content"]


def get_antonym(word, neg_token):
    antonyms = []
    for syn in wordnet.synsets(word):
        for lm in syn.lemmas():
            if lm.antonyms(): 
                antonyms.append(lm.antonyms()[0].name()) 
    if len(antonyms) != 0:
        return antonyms[0]
    else:
        return neg_token + "-" + word

def rpl_antonym(sentence, negative):
    for i in range(len(negative)):
        negative[i] = re.sub(r"[^a-zA-Z']+",'', negative[i])
    negative.sort(key=len)
    negative = negative[::-1]
    negative = list(unique_everseen(negative))
    sentence1 = sentence
    for j in range(len(negative)):
        words = sentence1.split()
        for i,w in enumerate(words):
            if w == negative[j]:
                idx = i+1
                if len(words) > idx:
                    words[idx] = get_antonym(words[i+1], negative[j])
        sentence1 = " ".join(words)
    sentence1 = [word for word in re.split("\W+",sentence1) if word not in negative]
    sentence1 = ' '.join(sentence1)
    return sentence1


writer = pd.ExcelWriter(r'final_news_cleaned_1911.xlsx')
news1.to_excel(writer,'Sheet1')
writer.save()



bank_names = list(alias_df.memberid)
negative=["no","not","rather","couldn’t","wasn’t","didn’t","wouldn’t","shouldn’t","weren’t","don’t","doesn’t","haven’t","hasn’t","won’t","wont","hadn’t","couldnt","wasnt","didnt","wouldnt","shouldnt","werent","dont","doesnt","havent","hasnt","hadnt","never","none","nobody","nothing","neither","nor","nowhere","isn’t","can’t","cannot","mustn’t","mightn’t","shan’t","without","needn’t"]
stopWords = set(stopwords.words('english') + bank_names + ["bank", "banks"])
contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", 
                   "can't've": "cannot have", "'cause": "because", "could've": "could have", 
                   "couldn't": "could not", "couldn't've": "could not have","didn't": "did not", 
                   "doesn't": "does not", "don't": "do not", "hadn't": "had not", 
                   "hadn't've": "had not have", "hasn't": "has not", "haven't": "have not", 
                   "he'd": "he would", "he'd've": "he would have", "he'll": "he will", 
                   "he'll've": "he will have", "he's": "he is", "how'd": "how did", 
                   "how'd'y": "how do you", "how'll": "how will", "how's": "how is", 
                   "I'd": "I would", "I'd've": "I would have", "I'll": "I will", 
                   "I'll've": "I will have","I'm": "I am", "I've": "I have", 
                   "i'd": "i would", "i'd've": "i would have", "i'll": "i will", 
                   "i'll've": "i will have","i'm": "i am", "i've": "i have", 
                   "isn't": "is not", "it'd": "it would", "it'd've": "it would have", 
                   "it'll": "it will", "it'll've": "it will have","it's": "it is", 
                   "let's": "let us", "ma'am": "madam", "mayn't": "may not", 
                   "might've": "might have","mightn't": "might not","mightn't've": "might not have", 
                   "must've": "must have", "mustn't": "must not", "mustn't've": "must not have", 
                   "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock", 
                   "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not",
                   "sha'n't": "shall not", "shan't've": "shall not have", "she'd": "she would", 
                   "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", 
                   "she's": "she is", "should've": "should have", "shouldn't": "should not", 
                   "shouldn't've": "should not have", "so've": "so have","so's": "so as", 
                   "this's": "this is",
                   "that'd": "that would", "that'd've": "that would have","that's": "that is", 
                   "there'd": "there would", "there'd've": "there would have","there's": "there is", 
                       "here's": "here is",
                   "they'd": "they would", "they'd've": "they would have", "they'll": "they will", 
                   "they'll've": "they will have", "they're": "they are", "they've": "they have", 
                   "to've": "to have", "wasn't": "was not", "we'd": "we would", 
                   "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", 
                   "we're": "we are", "we've": "we have", "weren't": "were not", 
                   "what'll": "what will", "what'll've": "what will have", "what're": "what are", 
                   "what's": "what is", "what've": "what have", "when's": "when is", 
                   "when've": "when have", "where'd": "where did", "where's": "where is", 
                   "where've": "where have", "who'll": "who will", "who'll've": "who will have", 
                   "who's": "who is", "who've": "who have", "why's": "why is", 
                   "why've": "why have", "will've": "will have", "won't": "will not", 
                   "won't've": "will not have", "would've": "would have", "wouldn't": "would not", 
                   "wouldn't've": "would not have", "y'all": "you all", "y'all'd": "you all would",
                   "y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",
                   "you'd": "you would", "you'd've": "you would have", "you'll": "you will", 
                   "you'll've": "you will have", "you're": "you are", "you've": "you have" } 


def preprocess(col, proper = True, urls=True, sing = True, expand = True, apos = True, punc = True, num = True, negate = True, stop = True, norm = True):
    #URLs removin.
    text = word_tokenize(col)
    tagged_sentence = pos_tag(text)
    edited_sentence =  [word for word, tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']
    col_proper = ' '.join(edited_sentence)
    col_1 = re.sub(r'^https?:\/\/.*[\r\n]','', col_proper)
    col_1 = re.sub(r'https?:\/\/','', col_1)
    col_1 = re.sub(r'\w*.com\b','', col_1)
    #URLs removed, saved in col_1
    #contraction handling
    apostrophe_handled = re.sub("’", "'", col_1)
    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(" ")])
    #Remove Punctuations
    col_2 = re.sub(r"[^a-zA-Z0-9'_]+",' ', expanded)
    col_2 = re.sub(r"\'s", "",col_2)
    #Puncuations removed, saved in col_2
    #Numbers removed, saved in col_3
    col_3 = rpl_antonym(col_2, negative)
    #Negation handled, saved in col_4
    #Remove Stopwords and entity names
    col_4 = [word for word in re.split("\W+",col_3) if word not in stopWords]
    col_4 = " ".join(col_4)
    #Stopwords and entity names removed, saved in col_5
    col_5 = re.sub(r'\d+','',col_4, flags = re.MULTILINE)
    #Lexicon Normalization
    tokens = word_tokenize(col_5)
    lemma_function = WordNetLemmatizer() 
    sent = []
    for token, tag in pos_tag(tokens):
        lemma = lemma_function.lemmatize(token)
        sent.append(lemma)
    col_6 = " ".join(sent)
    col_7 = col_6
    if sing:
        return col_7
    elif norm: 
        return col_6
    elif num:
        return col_5
    elif stop:
        return col_4
    elif negate:
        return col_3
    elif punc:
        return col_2
    elif expand:
        return expanded
    elif apos:
        return apostrophe_handled
    elif proper:
        return proper
    else:
        return col_1

news1["final_preprocessed"] = news1.merged_col.map(lambda x:preprocess(x))

news1['token_count'] = news1.merged_col.map(lambda x:len(x.split()))

news1 = news1.sort_values(by=['token_count'], ascending=True)

writer = pd.ExcelWriter(r'preprocessed_data.xlsx', options={'strings_to_urls': False})
news1.to_excel(writer,'Sheet1')
writer.save()

news1 = pd.read_excel(r"C:\Users\Mohit jetwal\Desktop\CCIL\preprocessed_data.xlsx", index_col = 0)

#Target encoding
news1['tag_new'] = news1['tag'].map(lambda x:x.lower())
news1['tag_new'] = news1['tag_new'].map({'normal': 0, 'caution': 1, 'alarming':2, 'catastrophic':2})



# Data Partitioning
random.seed(5)
train, test = train_test_split(news1, train_size = 0.7)


writer = pd.ExcelWriter(r'train.xlsx', options={'strings_to_urls': False})
train.to_excel(writer,'Sheet1')
writer.save()


writer = pd.ExcelWriter(r'test.xlsx', options={'strings_to_urls': False})
test.to_excel(writer,'Sheet1')
writer.save()


train = pd.read_excel(r"train.xlsx", index_col = 0)
test = pd.read_excel(r"test.xlsx", index_col = 0)


#Modelling
tvec = TfidfVectorizer(max_features = 1000, ngram_range=(1, 6))
#X and Y variables
X = train.final_preprocessed
Y = train.tag_new


smt = SMOTE(random_state=5)

vec = tvec.fit_transform(X)
vec_val = pd.DataFrame(vec.toarray())
vec_val.columns = tvec.get_feature_names()
X_SMOTE, Y_SMOTE = smt.fit_sample(vec_val, Y)
X_val = pd.DataFrame(X_SMOTE)
X_val.columns = tvec.get_feature_names()

freq_Y = Y.value_counts()
freq_YS = pd.Series(Y_SMOTE).value_counts()
weights = pd.Series(Y_SMOTE).map({0: freq_Y[0]/freq_YS[0], 1: freq_Y[1]/freq_YS[1], 2:freq_Y[2]/freq_YS[2]})



# Feature Reduction PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=500)
principalComponents = pd.DataFrame(pca.fit_transform(X_SMOTE))

#pickle dump pca
pickle.dump(pca, open( "pca.p", "wb" ))


# Validation on Hold out sample
X_test = test.final_preprocessed
Y_test = test.tag_new

X_test = tvec.transform(X_test)
X_test = pd.DataFrame(X_test.toarray())
X_test.columns = tvec.get_feature_names()
X_test =pca.transform(X_test)
X_test = pd.DataFrame(X_test)


# Hyperparameter tuning

  
# defining parameter range 
param_grid = {'C': [0.01, 0.001, 0.1, 1, 10, 100, 1000],  
              'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001, 'scale'], 
              'kernel': ['rbf', 'poly', 'linear'],
              'degree':[0,1,2,3,4,5]}  

grid = GridSearchCV(SVC(), param_grid, refit = True) 

# fitting the model for grid search 
grid.fit(principalComponents, Y_SMOTE, sample_weight = weights) 

print(grid.best_params_) 
print(grid.best_estimator_) 



model_svc = SVC(C=1,gamma=10,kernel = 'linear',degree = 5)

model_svc.fit(principalComponents, Y_SMOTE, sample_weight= weights)

with open('mod_svc.pkl', 'wb') as f:
    pickle.dump(model_svc, f)

with open('mod_svc.pkl', 'rb') as f:
    model_svc=pickle.load(f)


confusion_matrix(Y_SMOTE, model_svc.predict(principalComponents))
confusion_matrix(Y_test, model_svc.predict(X_test))

# fitting the model for grid search 
grid.fit(principalComponents, Y_SMOTE, sample_weight = weights) 

print(grid.best_params_) 
print(grid.best_estimator_) 

##SVM2

model_svc1 = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=2, gamma=10, kernel='poly',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)

model_svc1.fit(principalComponents, Y_SMOTE, sample_weight= weights)

with open('mod_svc.pkl', 'wb') as f:
    pickle.dump(model_svc, f)

with open('mod_svc.pkl', 'rb') as f:
    model_svc=pickle.load(f)


confusion_matrix(Y_SMOTE, model_svc1.predict(principalComponents))
confusion_matrix(Y_test, model_svc1.predict(X_test))
